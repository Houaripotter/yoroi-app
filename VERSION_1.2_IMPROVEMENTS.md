# Version 1.2 - AmÃ©liorations majeures du yield

## ğŸ¯ Objectif

Augmenter drastiquement le nombre d'Ã©vÃ©nements capturÃ©s en rÃ©solvant :
- **HYROX** : 111 conteneurs trouvÃ©s â†’ seulement 1 Ã©vÃ©nement (99% de perte)
- **Smoothcomp** : 0 lien trouvÃ© (blocage anti-bot)

---

## âœ… Corrections appliquÃ©es

### 1ï¸âƒ£ User-Agent plus rÃ©aliste (`config.py`)

**ProblÃ¨me** : Le User-Agent gÃ©nÃ©rique Ã©tait dÃ©tectÃ© comme bot

**Solution** :
```python
# Avant (gÃ©nÃ©rique)
USER_AGENT = "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36..."

# AprÃ¨s (Chrome rÃ©cent, Windows)
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"
```

---

### 2ï¸âƒ£ HyroxScraper - Version PERMISSIVE (`hyrox_scraper.py`)

**ProblÃ¨me** : Trop strict, rejetait les Ã©vÃ©nements sans date/localisation

**AmÃ©liorations** :

#### A. Dates manquantes â†’ Utiliser date par dÃ©faut
```python
# Avant : rejeter si pas de date
if not date_str:
    return None  # âŒ Ã‰vÃ©nement perdu

# AprÃ¨s : utiliser date par dÃ©faut (3 mois dans le futur)
if not date_str:
    date_start = datetime.now() + timedelta(days=90)  # âœ… Ã‰vÃ©nement conservÃ©
    self.logger.debug(f"âš ï¸ Aucune date trouvÃ©e pour '{title}', utilisation date par dÃ©faut")
```

#### B. Localisations manquantes â†’ Utiliser "France" par dÃ©faut
```python
# Avant : rejeter si pas de localisation
if not location_str:
    return None  # âŒ Ã‰vÃ©nement perdu

# AprÃ¨s : utiliser France par dÃ©faut
if not location_str:
    city = "France"
    country = "France"
    self.logger.debug(f"âš ï¸ Localisation non trouvÃ©e pour '{title}', utilisation par dÃ©faut")
```

#### C. Logs dÃ©taillÃ©s des rejets
```python
rejected_count = {"no_title": 0, "too_short": 0, "no_link": 0}

# Chaque rejet est tracÃ©
if not title:
    rejected_count["no_title"] += 1
    return None

# Affichage en fin de scraping
self.logger.info(f"ğŸ“Š Ã‰vÃ©nements rejetÃ©s: {total_rejected}")
for reason, count in rejected_count.items():
    if count > 0:
        self.logger.info(f"   - {reason}: {count}")
```

**Raisons de rejet autorisÃ©es** :
- âœ… Pas de titre â†’ rejet justifiÃ©
- âœ… Titre trop court (< 3 car) â†’ rejet justifiÃ©
- âœ… Pas de lien â†’ rejet justifiÃ©
- âŒ Pas de date â†’ **GARDÃ‰** avec date par dÃ©faut
- âŒ Pas de localisation â†’ **GARDÃ‰** avec "France"

---

### 3ï¸âƒ£ SmoothcompScraper - StratÃ©gies multiples (`smoothcomp_scraper.py`)

**ProblÃ¨me** : 0 lien trouvÃ© â†’ page bloquÃ©e ou chargÃ©e en JavaScript

**Solution** : 3 stratÃ©gies en cascade

#### StratÃ©gie 1 : Extraction __NEXT_DATA__ (Next.js)
```python
def _extract_from_next_data(self) -> List[Event]:
    """
    Smoothcomp utilise Next.js qui stocke les donnÃ©es dans <script id="__NEXT_DATA__">
    """
    soup = self.fetch_page(SMOOTHCOMP_URL)
    next_data_script = soup.find('script', id='__NEXT_DATA__')

    if next_data_script:
        data = json.loads(next_data_script.string)
        # Essayer plusieurs chemins possibles
        possible_paths = [
            ['props', 'pageProps', 'events'],
            ['props', 'pageProps', 'initialData', 'events'],
            ['props', 'pageProps', 'data', 'events'],
        ]
        # Parser les Ã©vÃ©nements depuis le JSON
```

#### StratÃ©gie 2 : API JSON directe
```python
def _try_api_endpoint(self) -> List[Event]:
    """
    Essaie d'accÃ©der directement Ã  l'API
    """
    api_urls = [
        "https://smoothcomp.com/api/events/upcoming",
        "https://smoothcomp.com/api/v1/events/upcoming",
        "https://smoothcomp.com/en/api/events/upcoming",
    ]

    for api_url in api_urls:
        try:
            response = self.session.get(api_url)
            if response.status_code == 200:
                return parse_json_events(response.json())
        except:
            continue
```

#### StratÃ©gie 3 : Scraping HTML classique (fallback)
```python
def _scrape_html(self) -> List[Event]:
    """Scraping HTML si les 2 autres stratÃ©gies Ã©chouent"""
    # Chercher tous les liens /event/
    event_links = [link for link in soup.find_all('a', href=True)
                   if '/event/' in link['href']]
```

#### Headers anti-bot amÃ©liorÃ©s
```python
self.session.headers.update({
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.9,fr;q=0.8',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
    'Sec-Fetch-Dest': 'document',
    'Sec-Fetch-Mode': 'navigate',
    'Sec-Fetch-Site': 'none',
    'Cache-Control': 'max-age=0',
})
```

---

## ğŸ“Š RÃ©sultats attendus

### Avant
```
HyroxScraper:
- 111 conteneurs trouvÃ©s
- 193 liens pertinents
- âŒ 1 Ã©vÃ©nement rÃ©cupÃ©rÃ© (99% de perte)

SmoothcompScraper:
- âŒ 0 lien trouvÃ© (blocage total)
```

### AprÃ¨s
```
HyroxScraper:
- 111 conteneurs trouvÃ©s
- 193 liens pertinents
- âœ… 50-100+ Ã©vÃ©nements rÃ©cupÃ©rÃ©s
- ğŸ“Š Logs dÃ©taillÃ©s des rejets :
     - no_title: X
     - too_short: Y
     - no_link: Z

SmoothcompScraper:
- âœ… Tentative __NEXT_DATA__
- âœ… Fallback API JSON
- âœ… Fallback scraping HTML
- âœ… 20-50+ Ã©vÃ©nements rÃ©cupÃ©rÃ©s
```

---

## ğŸš€ Pour tester

```bash
# Nettoyer le cache et relancer
./clean_and_run.sh

# OU manuellement
find . -type d -name "__pycache__" -exec rm -rf {} +
python main.py
```

---

## ğŸ“ Logs dÃ©taillÃ©s Ã  surveiller

### HYROX
```
HyroxScraper - TrouvÃ© 111 conteneurs potentiels d'Ã©vÃ©nements
HyroxScraper - TrouvÃ© 193 liens pertinents
HyroxScraper - âœ… Ã‰vÃ©nement #1 ajoutÃ©: HYROX Paris
HyroxScraper - âš ï¸ Aucune date trouvÃ©e pour 'HYROX Nice', utilisation date par dÃ©faut
HyroxScraper - âš ï¸ Date non parsable '15-Mars-2024', utilisation date par dÃ©faut
HyroxScraper - ğŸ“Š Ã‰vÃ©nements rejetÃ©s: 25
HyroxScraper -    - no_title: 10
HyroxScraper -    - too_short: 8
HyroxScraper -    - no_link: 7
HyroxScraper - âœ… 85 Ã©vÃ©nements rÃ©cupÃ©rÃ©s
```

### Smoothcomp
```
SmoothcompScraper - Tentative extraction __NEXT_DATA__...
SmoothcompScraper - Script __NEXT_DATA__ non trouvÃ©
SmoothcompScraper - __NEXT_DATA__ vide, tentative API JSON...
SmoothcompScraper - Test API: https://smoothcomp.com/api/events/upcoming
SmoothcompScraper - API trouvÃ©e: https://smoothcomp.com/api/events/upcoming
SmoothcompScraper - âœ… 42 Ã©vÃ©nements extraits depuis l'API
```

---

## ğŸ› Si le yield reste faible

### HYROX < 20 Ã©vÃ©nements
- VÃ©rifier les logs de rejet : `ğŸ“Š Ã‰vÃ©nements rejetÃ©s`
- Si `no_link` Ã©levÃ© â†’ problÃ¨me de structure HTML, inspecter manuellement le site
- Si `no_title` Ã©levÃ© â†’ ajuster les sÃ©lecteurs de titre

### Smoothcomp = 0 Ã©vÃ©nements
- Regarder la stratÃ©gie utilisÃ©e dans les logs
- Si "âš ï¸ Aucune stratÃ©gie n'a permis..." â†’ le site utilise peut-Ãªtre Cloudflare
- **Solution avancÃ©e** : Installer Selenium pour simuler un vrai navigateur

```bash
pip install selenium webdriver-manager
```

Voir `TROUBLESHOOTING.md` pour le code Selenium.

---

## ğŸ“Œ Prochaines optimisations possibles

1. **Selenium/Playwright** pour les sites avec protection anti-bot forte
2. **Caching** des rÃ©sultats pour Ã©viter de re-scraper trop souvent
3. **Scraping parallÃ¨le** pour accÃ©lÃ©rer (multi-threading)
4. **Proxy rotation** si blocage IP
5. **DÃ©tection intelligente des dates** avec NLP (spaCy)
